{"cells":[{"cell_type":"markdown","metadata":{"id":"nWSzCASuu6jx"},"source":["## Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8361,"status":"ok","timestamp":1702834403959,"user":{"displayName":"Ermiyas Birhanu","userId":"14009542705771269522"},"user_tz":-60},"id":"sSk0B9j3qQFz","outputId":"41b82bc4-9f3e-4655-eb8b-5641b00ecdc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dcor\n","  Downloading dcor-0.6-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dcor) (1.23.5)\n","Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.10/dist-packages (from dcor) (0.58.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dcor) (1.11.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dcor) (1.3.2)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51->dcor) (0.41.1)\n","Installing collected packages: dcor\n","Successfully installed dcor-0.6\n"]}],"source":["!pip install --upgrade dcor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22281,"status":"ok","timestamp":1701778932690,"user":{"displayName":"Ermiyas Birhanu","userId":"14009542705771269522"},"user_tz":-60},"id":"oPM1fGPin7sM","outputId":"c9e4bd60-ad8f-4dc9-e878-9da36b947b75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting minepy\n","  Downloading minepy-1.2.6.tar.gz (496 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/497.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/497.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.0/497.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from minepy) (1.23.5)\n","Building wheels for collected packages: minepy\n","  Building wheel for minepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for minepy: filename=minepy-1.2.6-cp310-cp310-linux_x86_64.whl size=187014 sha256=b7f097a5cad9af83a377f83b2efc79d85080267e51942d258ea694c1f2bc0702\n","  Stored in directory: /root/.cache/pip/wheels/69/38/a6/825bb9b9ed81e6af43a0ef80c7cfe4cafcfdbc2f5cde2959d9\n","Successfully built minepy\n","Installing collected packages: minepy\n","Successfully installed minepy-1.2.6\n"]}],"source":["!pip install minepy"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"wOANUSvIp4Bg","executionInfo":{"status":"ok","timestamp":1703389205160,"user_tz":-60,"elapsed":17815,"user":{"displayName":"Ermiyas Birhanu","userId":"14009542705771269522"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","\n","#saves variables\n","import pickle\n","#saves into files\n","from numpy import savetxt\n","\n","# # MIC correlation\n","# from minepy import MINE\n","\n","# # distance correlation\n","# import dcor\n","from scipy.spatial.distance import correlation\n","\n","\n","import matplotlib.pyplot  as plt\n","import seaborn as sns\n","# sns.set()\n","\n","# LSTM\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from matplotlib import pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import roc_curve, auc\n","\n","from scipy.stats import multivariate_normal\n","def xx(a,b):\n","  return (a+b)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wfV6CErnTpSW","executionInfo":{"status":"ok","timestamp":1703389208589,"user_tz":-60,"elapsed":765,"user":{"displayName":"Ermiyas Birhanu","userId":"14009542705771269522"}}},"outputs":[],"source":["class ADClass:\n","\n","    def __init__(self, dataset_path):\n","        \"\"\"\n","        Initialize dataset file path.\n","        \"\"\"\n","        self.dataset_path = dataset_path\n","        self.LCVs = []\n","        #self.df_categorical_fields = ['MV101', 'P101', 'P102', 'MV201', 'P201', 'P202', 'P203', 'P204', 'P205', 'P206', 'MV301',\n","         #                             'MV302', 'MV303', 'MV304', 'P301', 'P302', 'P401', 'P402', 'P403', 'P404', 'UV401', 'P501',\n","        #                            'P502', 'P601', 'P602', 'P603']\n","        self.df_numerical_fields = ['P1_B2004', 'P1_B2016', 'P1_B3004', 'P1_B3005', 'P1_B4002', 'P1_B4005', 'P1_B400B', 'P1_B4022',\n","                                    'P1_FCV01D', 'P1_FCV01Z', 'P1_FCV02D', 'P1_FCV02Z', 'P1_FCV03D', 'P1_FCV03Z', 'P1_FT01', 'P1_FT01Z', 'P1_FT02',\n","                                    'P1_FT02Z', 'P1_FT03', 'P1_FT03Z', 'P1_LCV01D', 'P1_LCV01Z', 'P1_LIT01', 'P1_PCV01D', 'P1_PCV01Z',\n","                                    'P1_PCV02Z', 'P1_PIT01', 'P1_PIT02', 'P1_TIT01', 'P1_TIT02', 'P2_24Vdc', 'P2_SD01',\n","                                    'P2_SIT01', 'P2_VT01e', 'P2_VXT02', 'P2_VXT03', 'P2_VYT02', 'P2_VYT03', 'P3_LCP01D', 'P3_LCV01D',\n","                                     'P3_LT01', 'P4_HT_FD', 'P4_HT_LD', 'P4_HT_PO','P4_LD', 'P4_ST_FD',\n","                                    'P4_ST_LD', 'P4_ST_PO', 'P4_ST_PS', 'P4_ST_PT01', 'P4_ST_TT01']\n","\n","\n","\n","\n","\n","    def importDataset(self, file_name, nb_rows):\n","        if nb_rows == 0:\n","          return pd.read_excel(self.dataset_path + file_name, header=0)\n","        else:\n","          return pd.read_excel(self.dataset_path + file_name, header=0, nrows=nb_rows)\n","\n","    def trimColumnName(slfe, df):\n","\n","        return df.rename(columns=lambda x: x.strip())\n","\n","    def splitDataset(self, df):\n","        x_n, y_n = df.iloc[:, :-1], df.iloc[:, [-1]]\n","\n","        return  x_n, y_n\n","\n","    def oneHotEncoding(self, fixed_value_list, x_n):\n","        new_cat_list = list(filter(lambda x: x not in fixed_value_list, self.df_categorical_fields))\n","\n","        #creating instance of one-hot-encoder\n","        encoder = OneHotEncoder(handle_unknown='ignore')\n","        # One-hot-encode the categorical columns.\n","        enc_package_type = pd.DataFrame(encoder.fit_transform(x_n[new_cat_list]).toarray())\n","        #merge one-hot encoded columns back with original DataFrame\n","        x_n = x_n.join(enc_package_type)\n","\n","        return x_n\n","\n","    def removeCategorical(self, fixed_value_list, x_n):\n","        new_cat_list = list(filter(lambda x: x not in fixed_value_list, self.df_categorical_fields))\n","        x_n.drop(new_cat_list, axis=1, inplace=True)\n","\n","        return x_n\n","\n","\n","    def createSequence(slef, x_n_scaled, window_size, shift):\n","        # Create sequences\n","        train_windows = []\n","        for i in range(0, x_n_scaled.shape[0] - window_size - shift, shift):\n","            train_windows.append(x_n_scaled[i:i+window_size])\n","\n","        # Convert to numpy array\n","        x_w_train = np.array(train_windows)\n","\n","        return x_w_train\n","\n","    def windowDetection_LSTM_AE(self, window_size_list, lstm_X_train, lstm_X_test, LSTM_w_shift):\n","        history_list = []\n","        mse_test = []\n","\n","        for i in window_size_list:\n","          window_size = i\n","\n","          # Create sequences\n","          train_windows = []\n","          test_windows = []\n","\n","          x_w_train = AD_normal.createSequence(lstm_X_train, window_size, LSTM_w_shift)\n","          x_w_test = AD_normal.createSequence(lstm_X_test, window_size, LSTM_w_shift)\n","\n","          # data dimensions // hyperparameters\n","          input_dim = x_w_train.shape[2]\n","          BATCH_SIZE = 256\n","          EPOCHS = 50\n","\n","          # create model\n","          LSTM_model = keras.Sequential()\n","          LSTM_model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', batch_input_shape=(None, window_size, input_dim), return_sequences=True, name='encoder_1'))\n","          LSTM_model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='encoder_2'))\n","          LSTM_model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=False, name='encoder_3'))\n","          LSTM_model.add(keras.layers.RepeatVector(window_size, name='encoder_decoder_bridge'))\n","          LSTM_model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=True, name='decoder_1'))\n","          LSTM_model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='decoder_2'))\n","          LSTM_model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', return_sequences=True, name='decoder_3'))\n","          LSTM_model.add(keras.layers.TimeDistributed(keras.layers.Dense(input_dim)))\n","\n","          early_stop = tf.keras.callbacks.EarlyStopping(\n","              monitor='val_loss',\n","              min_delta=1e-2,\n","              patience=5,\n","              verbose=1, mode='auto',\n","              baseline=None,\n","              restore_best_weights=True\n","          )\n","\n","\n","          # the default learning rate is used for the Adam optimizer, which is typically set to 0.001.\n","          LSTM_model.compile(optimizer=\"adam\",\n","                              loss=\"mse\",\n","                              metrics=[\"acc\"])\n","\n","          # print an overview of our model\n","          LSTM_model.summary();\n","\n","          history = LSTM_model.fit(\n","            x_w_train,\n","            x_w_train,\n","            epochs=50,\n","            batch_size=128,\n","            validation_split=0.2,\n","            callbacks=[\n","                early_stop\n","            ],\n","          )\n","\n","          plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n","          plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n","          plt.legend()\n","          plt.show()\n","\n","          # Generate predictions on the test set\n","          X_w_pred = LSTM_model.predict(x_w_test)\n","\n","          # Evaluate the autoencoder\n","          mse = np.mean(np.power(x_w_test - X_w_pred, 2), axis=1)\n","          print(\"Mean Squared Error:\", np.mean(mse))\n","          print()\n","\n","          # save values\n","          history_list.append(history)\n","          mse_test.append(np.mean(mse))\n","\n","        return history_list, mse_test\n","\n","\n","    # /*\n","    # transposing the window array is necessary to ensure that each row represents a\n","    # feature and each column represents a sample, which is the correct format for\n","    # calculating the correlation matrix using the np.corrcoef function.\n","\n","    # NOTE:\n","    # -----\n","    # Solutions to avoin NaN from deviding by zero\n","    # 1- To avoid the divide-by-zero problem when dealing with constant values.\n","    #    One such coefficient is the Spearman rank correlation coefficient.\n","    # 2- Add small amount of noise to the data.\n","    # */\n","    def calculateLCV(self, x_w_train):\n","\n","        LCVs = []\n","\n","\n","\n","        # sol.2 :\n","        for i in range(x_w_train.shape[0]):\n","          # Convert the window to a Pandas DataFrame\n","          window_df = pd.DataFrame(x_w_train[i])\n","\n","          # find constant columns using NumPy isclose() function\n","          constant_cols = [col for col in window_df.columns if np.isclose(window_df[col], window_df[col].iloc[0], rtol=1e-15, atol=1e-15).all()]\n","\n","          # Print the constant columns in the window\n","          # print(f\"Constant columns in 2nd window {i//window_size}: {constant_cols}\")\n","\n","          # Add a small amount of random noise to the constant columns\n","          window_df[constant_cols] += np.abs(np.random.normal(scale=0.00000001, size=window_df[constant_cols].shape))\n","\n","          # Compute the correlation matrix for the window\n","          corr_matrix = window_df.corr()\n","\n","          # Extract the upper triangle of the correlation matrix into a vector\n","          correlation_vector = corr_matrix.values[np.triu_indices_from(corr_matrix, k=1)]\n","\n","          ### DEBUG\n","          # print(window_df)\n","          # print(corr_matrix)\n","          # print(correlation_vector)\n","\n","          # from numpy import savetxt\n","          # # save to csv file\n","          # savetxt('window_df.csv', window_df, delimiter=',')\n","          # savetxt('corr_matrix_.csv', corr_matrix, delimiter=',')\n","          # savetxt('correlation_vector_.csv', correlation_vector, delimiter=',')\n","\n","          LCVs.append(correlation_vector)\n","          if not len(LCVs)%100:\n","            print(len(LCVs),'\\n')\n","\n","        return LCVs\n","\n","    def calculateLCV_spearman(self, x_w_train):\n","\n","        LCVs = []\n","\n","\n","        for i in range(x_w_train.shape[0]):\n","          # Convert the window to a Pandas DataFrame\n","          window_df = pd.DataFrame(x_w_train[i])\n","\n","          # find constant columns using NumPy isclose() function\n","          constant_cols = [col for col in window_df.columns if np.isclose(window_df[col], window_df[col].iloc[0], rtol=1e-15, atol=1e-15).all()]\n","\n","          # Print the constant columns in the window\n","          # print(f\"Constant columns in 2nd window {i//window_size}: {constant_cols}\")\n","\n","          # Add a small amount of random noise to the constant columns\n","          window_df[constant_cols] += np.abs(np.random.normal(scale=0.00000001, size=window_df[constant_cols].shape))\n","\n","          # Compute the correlation matrix for the window\n","          corr_matrix = window_df.corr(method='spearman')\n","\n","          # Extract the upper triangle of the correlation matrix into a vector\n","          correlation_vector = corr_matrix.values[np.triu_indices_from(corr_matrix, k=1)]\n","\n","          LCVs.append(correlation_vector)\n","          if not len(LCVs)%100:\n","            print(len(LCVs),'\\n')\n","\n","        return LCVs\n","\n","    def checkPosDef(self, x):\n","\n","        return np.all(np.linalg.eigvals(x) > 0)\n","\n","    def MGD(self, mean_vector, covariance_matrix):\n","\n","        mgd = multivariate_normal(mean=mean_vector, cov=covariance_matrix)\n","\n","        return mgd\n","\n","    def saveFile(self, file_name, var):\n","        # save to csv file\n","        savetxt(file_name, var, delimiter=',')\n","\n","        return\n","\n","    def calculateLCV_MIC(self, x_w_train):\n","\n","        LCVs = []\n","\n","        for k in range(x_w_train.shape[0]):\n","\n","          # Convert the window to a Pandas DataFrame\n","          window_df = pd.DataFrame(x_w_train[k])\n","          cols = window_df.columns\n","          n_cols = len(cols)\n","\n","          my_list = []\n","\n","          for i in range(n_cols):\n","              for j in range(i+1, n_cols):\n","                  col1, col2 = cols[i], cols[j]\n","                  mine = MINE()\n","                  mine.compute_score(window_df[col1], window_df[col2])\n","                  my_list.append(mine.mic())\n","\n","          my_array = np.array(my_list)\n","\n","          LCVs.append(my_array)\n","\n","          if not len(LCVs)%100:\n","            print(len(LCVs),'\\n')\n","\n","        return LCVs\n","\n","    def calculateLCV_DistnaceCorr(self, x_w_train):\n","\n","\n","        LCVs = []\n","        n_cols = x_w_train.shape[2]\n","\n","        for k in range(x_w_train.shape[0]):\n","            # Convert the window to a Pandas DataFrame\n","            window_df = pd.DataFrame(x_w_train[k])\n","            my_list = []\n","\n","            for i in range(n_cols):\n","                for j in range(i+1, n_cols):\n","                    col1, col2 = window_df.columns[i], window_df.columns[j]\n","                    dcor_ = dcor.distance_correlation(window_df[col1], window_df[col2])\n","                    # dcor_ = correlation(window_df[col1], window_df[col2], w=None, centered=True)\n","                    my_list.append(dcor_)\n","\n","            my_array = np.array(my_list)\n","            LCVs.append(my_array)\n","\n","            if len(LCVs) % 100 == 0:\n","                print(len(LCVs), '\\n')\n","\n","        return LCVs\n","\n","\n","    def getLabels(self, x_y_train):\n","\n","        LCVs_y = []\n","\n","        for i in range(x_y_train.shape[0]):\n","          # Convert the window to a Pandas DataFrame\n","          window_df = pd.DataFrame(x_y_train[i])\n","\n","          # Define the value you want to check for\n","          values_to_check = ['Attack', 'A ttack']\n","\n","          # Check if the value exists in the 'B' column\n","          if window_df[0].isin(values_to_check).any():\n","              # print(window_df[1].values)\n","              LCVs_y.append(1)\n","          else:\n","              # print(window_df[0].values)\n","              LCVs_y.append(0)\n","\n","\n","        return LCVs_y\n","\n","    def select_threshold(self, probs, test_data):\n","        best_epsilon = 0\n","        best_f1 = 0\n","        f = 0\n","        stepsize = (max(probs) - min(probs)) / 2000;\n","        epsilons = np.arange(min(probs), max(probs), stepsize)\n","        for epsilon in np.nditer(epsilons):\n","            predictions = (probs < epsilon)\n","            predictions\n","            f = f1_score(test_data, predictions, average='binary')\n","            # r = recall_score(test_data, predictions, average='binary')\n","            # Create confusion matrix\n","\n","            if f > best_f1:\n","                best_f1 = f\n","                best_epsilon = epsilon\n","\n","        # confusion matrix\n","        predictions = (probs < best_epsilon)\n","        cm = confusion_matrix(test_data, predictions)\n","\n","        return best_f1, best_epsilon, cm\n","\n","    def predictWithEpsilon(self, X, mvn, best_epsilon):\n","        predictions = []\n","\n","        pdf = mvn.logpdf(X)\n","        predictions = (pdf < best_epsilon)\n","\n","        return predictions, pdf\n","\n","    def predictWithAlpha(self, LCV_list, Sigma, mu, alpha):\n","        predictions = []\n","\n","        lower_bound = mu - alpha * np.diag(Sigma)\n","        upper_bound = mu + alpha * np.diag(Sigma)\n","\n","        for LCV in LCV_list:\n","          if (mu - alpha * np.diag(Sigma) <= LCV).all() and (LCV <= mu + alpha * np.diag(Sigma)).all():\n","              predictions.append(0)\n","          elif (LCV < lower_bound).any() or (LCV > upper_bound).any():\n","              predictions.append(1)\n","\n","        return predictions\n","\n","    def findBestAlpha(self, LCVs, ground_truth, Sigma, mu):\n","\n","        best_f1 = 0\n","        best_alpha = 0\n","        best_cm = None\n","\n","        alpha = 0\n","        step = 1\n","        # for alpha in range(0, 50, 0.2):\n","        while alpha < 250:\n","            predictions = self.predictWithAlpha(LCVs, Sigma, mu, alpha)\n","            f1 = f1_score(ground_truth, predictions, average='binary')\n","\n","            if f1 > best_f1:\n","                best_f1 = f1\n","                best_alpha = alpha\n","\n","            alpha += step\n","\n","        # confusion matrix\n","        predictions = self.predictWithAlpha(LCVs, Sigma, mu, best_alpha)\n","        cm = confusion_matrix(ground_truth, predictions)\n","\n","        return best_f1, best_alpha, cm\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Obv3xa-26JoK"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}